{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af3e5663",
   "metadata": {},
   "source": [
    "# Enhancing Fraud Detection Using Synthetic Transactions Generated by CTGAN\n",
    "\n",
    "**Project objective:** Create synthetic financial transaction data using CTGAN, augment real data, and improve fraud detection model performance.\n",
    "\n",
    "**Author:** Suryank\n",
    "\n",
    "**Timeline (4 weeks plan):**\n",
    "- Week 1: Problem understanding & EDA\n",
    "- Week 2: CTGAN training on minority class\n",
    "- Week 3: Data augmentation & ML training\n",
    "- Week 4: Evaluation, explainability & report\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a62690f",
   "metadata": {},
   "source": [
    "## 1. Setup: installs & imports\n",
    "\n",
    "This cell installs required libraries (if not present) and imports modules. If you're running offline, ensure required packages are already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24428b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the installs below if running in an environment with internet access\n",
    "# !pip install sdv ctgan==0.6.0 scikit-learn xgboost shap imbalanced-learn joblib\n",
    "# Note: sdv package includes CTGAN in newer versions, but ctgan package may be used in older tutorials.\n",
    "# If sdv installation fails, try: pip install ctgan\n",
    "\n",
    "import warnings, os, sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, roc_auc_score,\n",
    "                             classification_report, confusion_matrix, roc_curve, precision_recall_curve)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Optional imports (may not be installed in some runtimes)\n",
    "try:\n",
    "    from sdv.tabular import CTGAN  # sdv's CTGAN wrapper\n",
    "    has_ctgan = True\n",
    "except Exception as e:\n",
    "    try:\n",
    "        # fallback to ctgan package if sdv not available\n",
    "        from ctgan import CTGANSynthesizer as CTGAN_synth\n",
    "        has_ctgan = True\n",
    "    except Exception as e2:\n",
    "        has_ctgan = False\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBClassifier\n",
    "    has_xgb = True\n",
    "except Exception as e:\n",
    "    XGBClassifier = None\n",
    "    has_xgb = False\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    has_smote = True\n",
    "except Exception as e:\n",
    "    has_smote = False\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    has_shap = True\n",
    "except Exception as e:\n",
    "    has_shap = False\n",
    "\n",
    "import joblib\n",
    "\n",
    "print('CTGAN available:', has_ctgan, 'XGBoost available:', has_xgb, 'SMOTE available:', has_smote, 'SHAP available:', has_shap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67e1d39",
   "metadata": {},
   "source": [
    "## 2. Load dataset\n",
    "\n",
    "Place `creditcard.csv` (Kaggle Credit Card Fraud dataset) in the same directory. Adjust path if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc3249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'creditcard.csv'\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print('Dataset not found at', DATA_PATH, '\\nPlease upload the file and re-run.')\n",
    "else:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print('Loaded dataset with shape:', df.shape)\n",
    "    display(df.head())\n",
    "    display(df['Class'].value_counts())\n",
    "    print('\\nFraud ratio (Class=1): {:.6f}'.format(df['Class'].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e41f331",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "We will inspect distributions, correlations, PCA, and class imbalance. Document insights in markdown cells under each plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8988e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic EDA - run after dataset is loaded\n",
    "if 'df' in globals():\n",
    "    print('Columns:', df.columns.tolist())\n",
    "    print('\\nData types:')\n",
    "    display(df.dtypes.value_counts())\n",
    "\n",
    "    # Class distribution\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.countplot(x='Class', data=df)\n",
    "    plt.title('Class distribution (0: non-fraud, 1: fraud)')\n",
    "    plt.show()\n",
    "\n",
    "    # Histogram: Amount and Time\n",
    "    if 'Amount' in df.columns:\n",
    "        plt.figure(figsize=(12,4))\n",
    "        plt.subplot(1,2,1)\n",
    "        sns.histplot(df['Amount'], bins=50, kde=False)\n",
    "        plt.title('Transaction Amount distribution')\n",
    "        plt.subplot(1,2,2)\n",
    "        sns.boxplot(x=df['Amount'])\n",
    "        plt.title('Amount boxplot')\n",
    "        plt.show()\n",
    "\n",
    "    if 'Time' in df.columns:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.histplot(df['Time'], bins=50)\n",
    "        plt.title('Time distribution')\n",
    "        plt.show()\n",
    "\n",
    "    # Correlation heatmap for numeric features\n",
    "    numeric = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    plt.figure(figsize=(14,10))\n",
    "    sns.heatmap(df[numeric].corr(), cmap='coolwarm', center=0)\n",
    "    plt.title('Correlation heatmap')\n",
    "    plt.show()\n",
    "\n",
    "    # PCA 2D projection (sampled for speed)\n",
    "    sample = df.sample(n=min(2000, len(df)), random_state=42)\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    X_pca = pca.fit_transform(sample.drop('Class', axis=1))\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(X_pca[sample['Class']==0,0], X_pca[sample['Class']==0,1], label='non-fraud', alpha=0.3)\n",
    "    plt.scatter(X_pca[sample['Class']==1,0], X_pca[sample['Class']==1,1], label='fraud', alpha=0.6)\n",
    "    plt.legend(); plt.title('PCA 2D (sample)')\n",
    "    plt.show()\n",
    "\n",
    "    # t-SNE (on smaller sample) to visualize real vs synthetic later\n",
    "    # we'll compute t-SNE again after generating synthetic data\n",
    "else:\n",
    "    print('Load the dataset first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4d25d5",
   "metadata": {},
   "source": [
    "## 4. CTGAN training on minority class and synthetic data generation\n",
    "\n",
    "Train CTGAN on fraud (Class=1) examples to generate more fraudulent transactions for augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd56cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in globals():\n",
    "    fraud = df[df['Class']==1].drop(columns=['Class']).reset_index(drop=True)\n",
    "    nonfraud = df[df['Class']==0].drop(columns=['Class']).reset_index(drop=True)\n",
    "    print('Real fraud samples:', len(fraud), 'Non-fraud samples:', len(nonfraud))\n",
    "\n",
    "    if has_ctgan:\n",
    "        print('\\nTraining CTGAN on fraud-only data...')\n",
    "        try:\n",
    "            # sdv's CTGAN expects a table-like DataFrame and can accept continuous features\n",
    "            ctgan = None\n",
    "            from sdv.tabular import CTGAN as SDV_CTGAN\n",
    "            ctgan = SDV_CTGAN(epochs=100)\n",
    "            ctgan.fit(fraud)\n",
    "            # Generate synthetic fraud samples (adjust n as needed)\n",
    "            n_synth = min(5000, max(1000, len(fraud)*50))\n",
    "            synthetic_fraud = ctgan.sample(n_synth)\n",
    "            print('Generated synthetic fraud samples:', synthetic_fraud.shape)\n",
    "            display(synthetic_fraud.head())\n",
    "        except Exception as e:\n",
    "            print('Error training CTGAN from sdv:', e)\n",
    "            # Try fallback ctgan package\n",
    "            try:\n",
    "                from ctgan import CTGANSynthesizer\n",
    "                synth = CTGANSynthesizer(epochs=300)\n",
    "                synth.fit(fraud, discrete_columns=[])\n",
    "                synthetic_fraud = synth.sample(n_synth)\n",
    "                print('Generated synthetic fraud via ctgan package')\n",
    "            except Exception as e2:\n",
    "                print('CTGAN fallback failed:', e2)\n",
    "                synthetic_fraud = None\n",
    "    else:\n",
    "        print('CTGAN not available in this runtime. Please install sdv or ctgan to run this step.')\n",
    "        synthetic_fraud = None\n",
    "\n",
    "else:\n",
    "    print('Load dataset first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8b0195",
   "metadata": {},
   "source": [
    "### 4.1 Compare real vs synthetic fraud distributions\n",
    "\n",
    "Check summary statistics and some feature distributions for a few columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28252bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'synthetic_fraud' in globals() and synthetic_fraud is not None:\n",
    "    # numeric columns in fraud and synthetic\n",
    "    num_cols = synthetic_fraud.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    sample_cols = num_cols[:6]\n",
    "    print('Comparing columns:', sample_cols)\n",
    "    display(pd.concat([fraud[sample_cols].describe().T.add_prefix('real_'),\n",
    "                       synthetic_fraud[sample_cols].describe().T.add_prefix('synth_')], axis=1))\n",
    "\n",
    "    # KDE plots for first 3 numeric columns\n",
    "    for c in sample_cols[:3]:\n",
    "        plt.figure(figsize=(7,3))\n",
    "        sns.kdeplot(fraud[c], label='real_fraud', fill=True)\n",
    "        sns.kdeplot(synthetic_fraud[c], label='synthetic_fraud', fill=True)\n",
    "        plt.title('Real vs Synthetic distribution: '+c)\n",
    "        plt.legend(); plt.show()\n",
    "else:\n",
    "    print('No synthetic_fraud to compare; run CTGAN step or load synthetic_fraud variable.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57d25da",
   "metadata": {},
   "source": [
    "### 4.2 t-SNE: visualize real fraud vs synthetic fraud (2D)\n",
    "\n",
    "Combine small samples and run t-SNE to see overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f35ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'synthetic_fraud' in globals() and synthetic_fraud is not None:\n",
    "    # Sample real fraud and synthetic fraud for visualization\n",
    "    r_sample = fraud.sample(n=min(500, len(fraud)), random_state=42)\n",
    "    s_sample = synthetic_fraud.sample(n=min(500, len(synthetic_fraud)), random_state=42)\n",
    "    combined = pd.concat([r_sample, s_sample], ignore_index=True)\n",
    "    labels = np.array([0]*len(r_sample) + [1]*len(s_sample))  # 0=real,1=synth\n",
    "\n",
    "    # Scale before t-SNE\n",
    "    scaler_vis = StandardScaler()\n",
    "    combined_scaled = scaler_vis.fit_transform(combined)\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "    z = tsne.fit_transform(combined_scaled)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(z[labels==0,0], z[labels==0,1], label='real fraud', alpha=0.6)\n",
    "    plt.scatter(z[labels==1,0], z[labels==1,1], label='synthetic fraud', alpha=0.6)\n",
    "    plt.legend(); plt.title('t-SNE: real vs synthetic fraud (sample)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Synthetic fraud not available â€” run CTGAN step.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80ae2c",
   "metadata": {},
   "source": [
    "## 5. Data Augmentation & Preprocessing\n",
    "\n",
    "Combine original data with synthetic fraud, shuffle, encode and scale. Split into train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9149fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation - create augmented dataset if synthetic exists\n",
    "if 'df' in globals():\n",
    "    if 'synthetic_fraud' in globals() and synthetic_fraud is not None:\n",
    "        synthetic_fraud_copy = synthetic_fraud.copy()\n",
    "        synthetic_fraud_copy['Class'] = 1\n",
    "        augmented = pd.concat([df, synthetic_fraud_copy], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        print('Augmented shape:', augmented.shape)\n",
    "    else:\n",
    "        print('No synthetic - using original dataset only')\n",
    "        augmented = df.copy()\n",
    "\n",
    "    # Prepare X,y\n",
    "    X = augmented.drop(columns=['Class'])\n",
    "    y = augmented['Class']\n",
    "\n",
    "    # One-hot encode if categorical present (creditcard dataset is numeric)\n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "    # Train-test split (stratify if possible)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    print('Train size:', X_train.shape, 'Test size:', X_test.shape)\n",
    "\n",
    "    # Scale numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Optionally apply SMOTE on training set (compare with augmented approach)\n",
    "    if has_smote:\n",
    "        sm = SMOTE(random_state=42)\n",
    "        X_train_sm, y_train_sm = sm.fit_resample(X_train_scaled, y_train)\n",
    "        print('After SMOTE, train size:', X_train_sm.shape, 'Class dist:', pd.Series(y_train_sm).value_counts().to_dict())\n",
    "    else:\n",
    "        X_train_sm, y_train_sm = X_train_scaled, y_train\n",
    "else:\n",
    "    print('Load dataset first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cc4355",
   "metadata": {},
   "source": [
    "## 6. Model training: Baselines & comparisons\n",
    "\n",
    "Train Logistic Regression, Random Forest, and XGBoost (if available). Provide functions to compute metrics and plot confusion matrix/ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b06b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_metrics(y_true, y_pred, y_prob):\n",
    "    return {\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_true, y_prob) if len(np.unique(y_true))>1 else np.nan\n",
    "    }\n",
    "\n",
    "def plot_confusion(y_true, y_pred, title='Confusion Matrix'):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title); plt.xlabel('Predicted'); plt.ylabel('Actual'); plt.show()\n",
    "\n",
    "models_results = []\n",
    "\n",
    "# Logistic Regression (baseline)\n",
    "if 'X_train_scaled' in globals():\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    y_pred_lr = lr.predict(X_test_scaled)\n",
    "    y_prob_lr = lr.predict_proba(X_test_scaled)[:,1]\n",
    "    m_lr = get_metrics(y_test, y_pred_lr, y_prob_lr)\n",
    "    models_results.append(('LogisticRegression', m_lr, lr))\n",
    "    print('Logistic Regression metrics:', m_lr)\n",
    "    plot_confusion(y_test, y_pred_lr, 'Logistic Regression CM')\n",
    "else:\n",
    "    print('Preprocessing required')\n",
    "\n",
    "# Random Forest (trained on augmented data)\n",
    "if 'X_train_scaled' in globals():\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    y_pred_rf = rf.predict(X_test_scaled)\n",
    "    y_prob_rf = rf.predict_proba(X_test_scaled)[:,1]\n",
    "    m_rf = get_metrics(y_test, y_pred_rf, y_prob_rf)\n",
    "    models_results.append(('RandomForest_augmented', m_rf, rf))\n",
    "    print('Random Forest (augmented) metrics:', m_rf)\n",
    "    plot_confusion(y_test, y_pred_rf, 'Random Forest (augmented) CM')\n",
    "\n",
    "# Random Forest trained on SMOTE-resampled training set (if SMOTE used)\n",
    "if has_smote and 'X_train_sm' in globals():\n",
    "    rf_sm = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "    rf_sm.fit(X_train_sm, y_train_sm)\n",
    "    y_pred_rf_sm = rf_sm.predict(X_test_scaled)\n",
    "    y_prob_rf_sm = rf_sm.predict_proba(X_test_scaled)[:,1]\n",
    "    m_rf_sm = get_metrics(y_test, y_pred_rf_sm, y_prob_rf_sm)\n",
    "    models_results.append(('RandomForest_SMOTE', m_rf_sm, rf_sm))\n",
    "    print('Random Forest (SMOTE-train) metrics:', m_rf_sm)\n",
    "    plot_confusion(y_test, y_pred_rf_sm, 'Random Forest (SMOTE) CM')\n",
    "\n",
    "# XGBoost (if available)\n",
    "if has_xgb and 'X_train_scaled' in globals():\n",
    "    xgbc = XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=200, random_state=42)\n",
    "    xgbc.fit(X_train_scaled, y_train)\n",
    "    y_pred_xgb = xgbc.predict(X_test_scaled)\n",
    "    y_prob_xgb = xgbc.predict_proba(X_test_scaled)[:,1]\n",
    "    m_xgb = get_metrics(y_test, y_pred_xgb, y_prob_xgb)\n",
    "    models_results.append(('XGBoost_augmented', m_xgb, xgbc))\n",
    "    print('XGBoost metrics:', m_xgb)\n",
    "    plot_confusion(y_test, y_pred_xgb, 'XGBoost CM')\n",
    "\n",
    "# Summary table\n",
    "summary_rows = []\n",
    "for name, metrics, _ in models_results:\n",
    "    row = {'model': name}\n",
    "    row.update(metrics)\n",
    "    summary_rows.append(row)\n",
    "\n",
    "if summary_rows:\n",
    "    summary_df = pd.DataFrame(summary_rows).set_index('model')\n",
    "    display(summary_df)\n",
    "else:\n",
    "    print('No models trained yet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc592646",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter tuning (Random Forest)\n",
    "\n",
    "Use RandomizedSearchCV to tune a Random Forest trained on augmented (or SMOTE) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "if 'X_train_sm' in globals():\n",
    "    param_dist = {\n",
    "        'n_estimators': sp_randint(100, 500),\n",
    "        'max_depth': sp_randint(3, 30),\n",
    "        'min_samples_split': sp_randint(2, 20),\n",
    "        'min_samples_leaf': sp_randint(1, 10)\n",
    "    }\n",
    "    rf_base = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    rs = RandomizedSearchCV(rf_base, param_distributions=param_dist, n_iter=20, cv=3, scoring='roc_auc', random_state=42, n_jobs=-1)\n",
    "    rs.fit(X_train_sm, y_train_sm)\n",
    "    print('Best params:', rs.best_params_)\n",
    "    best_rf = rs.best_estimator_\n",
    "    y_pred_best = best_rf.predict(X_test_scaled)\n",
    "    y_prob_best = best_rf.predict_proba(X_test_scaled)[:,1]\n",
    "    print('Tuned RF metrics:', get_metrics(y_test, y_pred_best, y_prob_best))\n",
    "    plot_confusion(y_test, y_pred_best, 'Tuned RF CM')\n",
    "else:\n",
    "    print('SMOTE data not available; tuning on X_train_sm required (or adjust to use X_train_scaled & y_train)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ddf9c9",
   "metadata": {},
   "source": [
    "## 8. Evaluation: ROC and Precision-Recall curves\n",
    "\n",
    "Plot ROC and PR curves for available models to compare trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0404ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "for name, metrics, model in models_results:\n",
    "    # attempt to get probability outputs\n",
    "    try:\n",
    "        y_prob = model.predict_proba(X_test_scaled)[:,1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC={{:.3f}})\".format(auc))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC Curves'); plt.legend(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for name, metrics, model in models_results:\n",
    "    try:\n",
    "        y_prob = model.predict_proba(X_test_scaled)[:,1]\n",
    "        p, r, _ = precision_recall_curve(y_test, y_prob)\n",
    "        plt.plot(r, p, label=name)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precision-Recall Curves'); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f5c40d",
   "metadata": {},
   "source": [
    "## 9. t-SNE: Real non-fraud vs Real fraud vs Synthetic fraud (if synthetic exists)\n",
    "\n",
    "This visual helps check how close synthetic fraud is to real fraud and to non-fraud distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'synthetic_fraud' in globals() and synthetic_fraud is not None:\n",
    "    # sample sets\n",
    "    nonf = nonfraud.sample(n=min(1000, len(nonfraud)), random_state=42)\n",
    "    rf_real = fraud.sample(n=min(500, len(fraud)), random_state=42)\n",
    "    rf_synth = synthetic_fraud.sample(n=min(500, len(synthetic_fraud)), random_state=42)\n",
    "\n",
    "    combined = pd.concat([nonf, rf_real, rf_synth], ignore_index=True)\n",
    "    labels = (['nonfraud']*len(nonf)) + (['real_fraud']*len(rf_real)) + (['synth_fraud']*len(rf_synth))\n",
    "\n",
    "    # scale\n",
    "    combined_scaled = StandardScaler().fit_transform(combined)\n",
    "    tsne = TSNE(n_components=2, random_state=42, n_iter=1000, perplexity=40)\n",
    "    z = tsne.fit_transform(combined_scaled)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    for lbl in set(labels):\n",
    "        idx = [i for i,l in enumerate(labels) if l==lbl]\n",
    "        plt.scatter(z[idx,0], z[idx,1], label=lbl, alpha=0.6)\n",
    "    plt.legend(); plt.title('t-SNE: nonfraud vs real fraud vs synthetic fraud')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Synthetic fraud not available; run CTGAN step to generate synthetic_fraud')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543cd4d1",
   "metadata": {},
   "source": [
    "## 10. Model explainability with SHAP (optional)\n",
    "\n",
    "Use SHAP to interpret the best performing tree model. SHAP may be slow; sample for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ac80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_shap:\n",
    "    # pick a tree-based model if available\n",
    "    model_shap = None\n",
    "    for name, metrics, model in models_results:\n",
    "        if 'RandomForest' in name or 'XGBoost' in name:\n",
    "            model_shap = model\n",
    "            break\n",
    "    if 'best_rf' in globals():\n",
    "        model_shap = best_rf\n",
    "\n",
    "    if model_shap is not None:\n",
    "        explainer = shap.TreeExplainer(model_shap)\n",
    "        X_sample = pd.DataFrame(X_test_scaled, columns=X_test.columns).sample(n=min(200, X_test.shape[0]), random_state=42)\n",
    "        shap_values = explainer.shap_values(X_sample)\n",
    "        print('SHAP summary plot:')\n",
    "        shap.summary_plot(shap_values, X_sample)\n",
    "    else:\n",
    "        print('No tree-based model available for SHAP in this session')\n",
    "else:\n",
    "    print('SHAP not installed. To enable: pip install shap')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af903171",
   "metadata": {},
   "source": [
    "## 11. Save best model & scaler\n",
    "\n",
    "Persist best performing model (choose tuned RF or the best in models_results) and scaler for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model_to_save heuristically: best_rf > rf_sm > rf > xgb > lr\n",
    "model_to_save = None\n",
    "if 'best_rf' in globals():\n",
    "    model_to_save = best_rf\n",
    "else:\n",
    "    for name, metrics, model in models_results:\n",
    "        if 'RandomForest' in name:\n",
    "            model_to_save = model\n",
    "            break\n",
    "    if model_to_save is None and models_results:\n",
    "        model_to_save = models_results[0][2]\n",
    "\n",
    "if model_to_save is not None and 'scaler' in globals():\n",
    "    joblib.dump(model_to_save, 'enhanced_fraud_model.pkl')\n",
    "    joblib.dump(scaler, 'scaler.pkl')\n",
    "    print('Saved model to enhanced_fraud_model.pkl and scaler.pkl')\n",
    "else:\n",
    "    print('No model or scaler available to save')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8cafaf",
   "metadata": {},
   "source": [
    "## 12. Streamlit demo (save as `app.py`)\n",
    "\n",
    "A minimal Streamlit app to load model & scaler and predict fraud probability. Edit the input fields to match your features (this example assumes `Amount` and `Time` only for demo purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7673d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit_code = '''\n",
    "import streamlit as st\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "model = joblib.load('enhanced_fraud_model.pkl')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "st.title('Fraud detection demo')\n",
    "# Example inputs - adjust to your actual feature set\n",
    "amount = st.number_input('Amount', min_value=0.0, value=10.0)\n",
    "time = st.number_input('Time', min_value=0.0, value=10000.0)\n",
    "\n",
    "input_df = pd.DataFrame({'Amount':[amount], 'Time':[time]})\n",
    "# If your model expects more features, create them here or use a saved pipeline\n",
    "input_scaled = scaler.transform(input_df)\n",
    "prob = model.predict_proba(input_scaled)[:,1][0]\n",
    "st.write('Predicted fraud probability:', prob)\n",
    "\n",
    "if prob > 0.5:\n",
    "    st.warning('High fraud probability')\n",
    "else:\n",
    "    st.success('Low fraud probability')\n",
    "'''\n",
    "with open('app.py','w') as f:\n",
    "    f.write(streamlit_code)\n",
    "print('Wrote app.py for Streamlit demo. Run: streamlit run app.py (after adjusting inputs to match features)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57533a76",
   "metadata": {},
   "source": [
    "## 13. Conclusion & Future Work\n",
    "\n",
    "**Conclusion:**\n",
    "- Trained CTGAN on fraud-only data to generate synthetic fraudulent transactions.\n",
    "- Augmented the original dataset and trained multiple models (Logistic Regression, Random Forest, XGBoost).\n",
    "- Evaluated performance with ROC/AUC, precision, recall, F1 and visualized with ROC/PR and t-SNE.\n",
    "- Saved the best model and provided a Streamlit demo template.\n",
    "\n",
    "**Future work:**\n",
    "- Experiment with TimeGAN for sequential transaction modeling.\n",
    "- Try conditional generation and feature-aware CTGAN settings.\n",
    "- Deploy model as a REST API with FastAPI or Flask and integrate with monitoring.\n",
    "- Calibrate probabilities and add threshold tuning for production recall/precision trade-offs.\n",
    "\n",
    "---\n",
    "\n",
    "**Deliverables (suggested):**\n",
    "- Jupyter Notebook (this file)\n",
    "- `enhanced_fraud_model.pkl` and `scaler.pkl` after running the notebook\n",
    "- `app.py` Streamlit demo\n",
    "- Short report summarizing improvement with numbers & plots\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
